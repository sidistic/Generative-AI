{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaEWb_A07-dz",
        "outputId": "566109f8-5e37-496f-e882-58ff30b17f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-07 22:30:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-06-07 22:30:05 (151 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "kwEeq-779cRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#hyperparameters\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Here are all the unique characters in the text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\"\"\"We need to tokenize the characters before we can train the model\"\"\"\n",
        "\n",
        "# create mapping from char to int and vice versa\n",
        "stoi = { ch:i for i,ch in enumerate(chars)}\n",
        "itos = { i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "# encode and decode using the mapping above\n",
        "encode = lambda s: [stoi[c] for c in s] # takes a string and output is list of int\n",
        "decode = lambda l: ''.join(itos[i] for i in l) # takes list of int, outputs string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "# creating test train split\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    #generate a small batch of data for inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    x = torch.stack([data[i:block_size+i] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x,y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X,Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x) #(B, T, C)\n",
        "        q = self.query(x) #(B, T, C)\n",
        "\n",
        "        #Compute attention scores(\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C ** -0.5 #(B, T, C) @ (B, C, T) --> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) #(B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        #perform the weighted aggregation of the values\n",
        "        v = self.value(x) #(B, T, C)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd,n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4* n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embd, n_embd), # projection layer\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer Block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd//n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd) # normalizes feature across row\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential( *[Block(n_embd,n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx,  targets = None):\n",
        "        B,T = idx.shape\n",
        "\n",
        "        #idx and targets are both (B, T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device= device)) #(T,C)\n",
        "        x = tok_emb + pos_emb # (B, T, C)\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x) #(B, T, Vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T,C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            #crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # Sample from distribution\n",
        "            idx_next = torch.multinomial(probs,num_samples=1) # (B, 1)\n",
        "            # Append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-03)\n",
        "\n",
        "\n",
        "for iter in tqdm(range(max_iters)):\n",
        "\n",
        "    # Sanity check\n",
        "    # if iter % eval_interval == 0:\n",
        "    #     losses = estimate_loss()\n",
        "    #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    #sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    #evaluate the loss\n",
        "    logits, loss = m(xb,yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#generate from the model\n",
        "context = torch.zeros((1, 1), dtype= torch.long, device = device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc3-Am538H-Q",
        "outputId": "92eeef45-277d-431f-fdbe-80c722ff45aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [10:00<00:00,  8.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hncily of those hath by city's labours on thy hand;\n",
            "And all that shrift while I use myself,\n",
            "I shall not do it, see how 'tis doubtful.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "O, perpetual, disgraceive,\n",
            "Tongue with those extremestate doves!\n",
            "'O, let them fair before their watchment.\n",
            "\n",
            "LUCIO:\n",
            "Good times, I grieve thee to hear them thence!\n",
            "Transpiringly, their ends them they stand good on their hearts\n",
            "Flately and nothing. Their motions,\n",
            "The steed of issuit of city and odds!\n",
            "\n",
            "ESCALUS:\n",
            "How loving with griof the dikest, house\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_j0eI8SCJaV",
        "outputId": "fd1fa097-fbb6-4892-c433-9834694588f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "But not be invention'd, oratory,\n",
            "Our kindres' toys offer with prince,\n",
            "When in a vile of dream, when was 'O, for what's groans!'\n",
            "Torms!\n",
            "I'ld no joy. Would have thought me stiff like to a rogue\n",
            "As wish an assistering my will be\n",
            "Even in years.\n",
            "\n",
            "LEMDIO:\n",
            "No faith,\n",
            "Unless thou art the field age; no;\n",
            "For within the trespassion of the rote\n",
            "Divy who the duke that sword the youth of my false,\n",
            "Brothers, there's mockingth out off with thy breast,\n",
            "And spurn at all thy body tears to no mome\n",
            "And indeed, wisel men's son to delay:\n",
            "Then, hereafter brave that is chief and cried.\n",
            "Consider Your high Tator, took flow him 'tis,\n",
            "Though no month 's debt by his preggary 'nois;'\n",
            "His nose couration,\n",
            "And by ther he stood together, I, he did thee know\n",
            "Of divided against, yet thou canst becomes for a time\n",
            "That may convenient great and pear.\n",
            "\n",
            "SICINIUS:\n",
            "Every tricks,\n",
            "The terrible on his mother are, I would not\n",
            "You like to do find that t know away to my pite\n",
            "And I what he would do holesome.\n",
            "\n",
            "SICINIUS:\n",
            "You are deserve?\n",
            "You have sent for prizes\n",
            "And help mistrick: down dismisses a\n",
            "like monarch and hance to lead for.\n",
            "\n",
            "SICINIUS:\n",
            "Where have the son?\n",
            "\n",
            "CORIOLANUS:\n",
            "I mean to do him to present at the tomb.\n",
            "\n",
            "TITUS:\n",
            "Dire above him o' Orderlate\n",
            "The gardess of all the batternal point;\n",
            "What laught on; or if't, now repent 'tis doubted\n",
            "I from the trial of Rome.\n",
            "\n",
            "MENENIUS:\n",
            "Is that it?\n",
            "\n",
            "CORIOLANUS:\n",
            "How?\n",
            "\n",
            "Thillow ill talk the prophecy man but to Rome foolare\n",
            "I as as a house as surfidius as the sighs;\n",
            "The water shorted with\n",
            "Supple muling after,\n",
            "As the infectious they have to do: not know\n",
            "Why to do?\n",
            "\n",
            "EOVOLIXENES:\n",
            "Too friends, it to be time that he are cross:\n",
            "he less ere they to send for Rome\n",
            "That was here to this bill-gold here auther.\n",
            "\n",
            "JULIET:\n",
            "Have seen how use they off will the glass of\n",
            "Were late measies since juries counters.\n",
            "Ours, Jeruli!\n",
            "\n",
            "CLARENCE:\n",
            "How goes thy noble; and looks dearly\n",
            "full ocean assistition, that though what therefore art\n",
            "love have vanquish'd,\n",
            "Nined for grassing together window,\n",
            "Waves another finger'd than work\n",
            "Was tongue than we like a tyrand that? See.\n",
            "Why, let his command of watch, by deputy;\n",
            "a much makerness that offences me is;\n",
            "The trest rascour of a fearful holps\n",
            "Where is dangerous and haver\n",
            "Apparess the high of their sun.\n",
            "How at thy upherd's eyesighs gift of loy,\n",
            "Seer in the dropp'd sighs death on thee;\n",
            "Who laten through upon, clock'd to seel it Richard,\n",
            "And, play, love's princely to with peace.\n",
            "What, thed, what is man imagic and?\n",
            "What sten her would have crosses hard, that me,\n",
            "I would not we make amench in sorrow,\n",
            "By the false shop: ut, as words up for the loss,\n",
            "When the state of old eyesugal teaches.\n",
            "O, comes him hither brief, and by reside\n",
            "Too drown his penemies from the bloody ears?\n",
            "\n",
            "AWCLWISS:\n",
            "Nature hat.\n",
            "Women already? raises she may:\n",
            "There is what come to speak with a grue?\n",
            "\n",
            "Musician:\n",
            "An't if thou were made forsworn his body.\n",
            "\n",
            "Post:\n",
            "Your part and he may before: this inte itself\n",
            "The our purpose battle, great paints;\n",
            "For 'tis a kiss so quivial Jove of the like.\n",
            "\n",
            "WARWICK:\n",
            "What, my lord: what and prince hath many child\n",
            "Lords Lord Hastings, whose lord, whose arms\n",
            "Draw histors shall play it, with though sits drew siots,\n",
            "Where told on the world them sunsel's,\n",
            "on thou shed should clage, if thou canst do count hatch us\n",
            "The nature of Hrosby and thy shaloss wizes thy side!\n",
            "Little calling clouds and lions of true:\n",
            "Revolving in death, though in once,\n",
            "Had serve at wars and peaceful partiest-frails;\n",
            "Even of any of jument, as his tumble,\n",
            "And tell'd thou hadst wrink against in our ill!\n",
            "Tear thou weed'st and create she begin of hear.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "What!\n",
            "Dare stings! but me he knows pastimes.\n",
            "Hast not sported us hath continued his bastard?\n",
            "To best me of old pause hithers with a stail;\n",
            "And thou, if these best'st in wearing noble,\n",
            "Transiders, Lord Anrels' and slain Englishshes,\n",
            "Thy slowers dids but wash him for the frozen,\n",
            "Which thou seeks no sorrow: by whom I would I\n",
            "Was not in revenged by order\n",
            "At in your youth, fortunate:\n",
            "I come his her and your-word's sleep,\n",
            "But you should have we lose redress of mine.\n",
            "\n",
            "QUEEN ELIZABUCKIO:\n",
            "Seize that he was not cause these are full offence?\n",
            "\n",
            "Messenger thin:\n",
            "I promised he will not be absent.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Ah, I pray thee, deliver, it thou dost\n",
            "Was ever the facture of made, more oncensire\n",
            "Are men: I will not show 'tis as well as past\n",
            "Become to generation, cast so seemedam, as took.\n",
            "\n",
            "MARCIUS:\n",
            "Farewell: when you peer upon his feasts? keep to me\n",
            "From thwack their ends.\n",
            "\n",
            "CORIOLANUS:\n",
            "Thou art ask'd upon Corioli to be then a figured\n",
            "Than you passes your hear in his mile--throne, and the\n",
            "ape-i again: o' so his master's pates.\n",
            "\n",
            "ROMEO:\n",
            "I pretemest well, poor quity; to what love is.\n",
            "Before. say thou mercy: not thee are specially,\n",
            "how lack mock at onasting-climately; I spit,\n",
            "wipes and saddest-like men thunded we come out.\n",
            "\n",
            "MENENIUS:\n",
            "His bond of the sole town insuls: is if well\n",
            "we here eyes unseen us, Jainta's patching.\n",
            "\n",
            "CORIOLANUS:\n",
            "Herese of Aufive as heaven still pass\n",
            "Reported untimely; nails too prink one\n",
            "The doth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(m.state_dict(), 'device_model_weights.pth')"
      ],
      "metadata": {
        "id": "g1Pz131eDJ2g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}